{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "The languages each model supports:\n",
        "\n",
        "1. facebook/hf-seamless-m4t-medium\n",
        "\n",
        "here you can find the supported languages:\n",
        "\n",
        "https://huggingface.co/facebook/hf-seamless-m4t-medium/blob/main/tokenizer_config.json#L1887-L2089\n",
        "\n",
        "2. Helsinki-NLP/opus-mt-en-roa\n",
        "\n",
        "here you can find the supported languages:\n",
        "\n",
        "https://huggingface.co/Helsinki-NLP/opus-mt-en-roa\n",
        "\n",
        "3. facebook/nllb-200-distilled-600M\n",
        "\n",
        "here you can find the supported languages:\n",
        "\n",
        "https://huggingface.co/facebook/nllb-200-distilled-600M\n",
        "\n",
        "4. google/madlad400-10b-mt\n",
        "\n",
        "here you can find the supported languages:\n",
        "\n",
        "https://huggingface.co/google/madlad400-10b-mt"
      ],
      "metadata": {
        "id": "1aUba_eFsjIw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langid"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K8YECvJLbymG",
        "outputId": "019c0f95-5c45-41e3-a8a3-f775bc293aa7"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langid\n",
            "  Downloading langid-1.1.6.tar.gz (1.9 MB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.9 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.6/1.9 MB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m29.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from langid) (1.26.4)\n",
            "Building wheels for collected packages: langid\n",
            "  Building wheel for langid (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for langid: filename=langid-1.1.6-py3-none-any.whl size=1941171 sha256=3b4eccc480bff8288f232db8882c4d1eedb5aa5a7aa083afd92c2bb2cf942dad\n",
            "  Stored in directory: /root/.cache/pip/wheels/23/c8/c6/eed80894918490a175677414d40bd7c851413bbe03d4856c3c\n",
            "Successfully built langid\n",
            "Installing collected packages: langid\n",
            "Successfully installed langid-1.1.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import SeamlessM4TModel, AutoProcessor\n",
        "from transformers import AutoTokenizer, MarianMTModel\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
        "import torch\n",
        "import langid\n",
        "\n",
        "def translate_text():\n",
        "    # Get input text from the user\n",
        "    input_text = input(\"Enter the text you want to translate: \")\n",
        "\n",
        "    # Detect the input language using langid\n",
        "    input_language, _ = langid.classify(input_text)\n",
        "    print(f\"Detected input language: {input_language}\")\n",
        "\n",
        "    # Get the output language from the user\n",
        "    output_language = input(\"Enter the language code you want to translate to (e.g., 'fra' for French): \")\n",
        "\n",
        "    # List of available models\n",
        "    model_choices = [\n",
        "        \"facebook/hf-seamless-m4t-medium\",\n",
        "        \"Helsinki-NLP/opus-mt-en-roa\",\n",
        "        \"facebook/nllb-200-distilled-600M\",\n",
        "        \"google/madlad400-10b-mt\"\n",
        "    ]\n",
        "\n",
        "    # Display the model options to the user\n",
        "    print(\"\\nAvailable models:\")\n",
        "    for i, model in enumerate(model_choices, 1):\n",
        "        print(f\"{i}. {model}\")\n",
        "\n",
        "    # Get the model selection from the user\n",
        "    model_index = int(input(\"Select the translation model by number: \")) - 1\n",
        "\n",
        "    model_name = model_choices[model_index]\n",
        "    print(f\"\\nYou have selected the model: {model_name}\")\n",
        "\n",
        "    if model_index == 0:\n",
        "      # Load the pre-trained SeamlessM4T model and processor\n",
        "      model = SeamlessM4TModel.from_pretrained(\"facebook/hf-seamless-m4t-medium\")\n",
        "      processor = AutoProcessor.from_pretrained(\"facebook/hf-seamless-m4t-medium\")\n",
        "\n",
        "      # Check if CUDA is available and set the device accordingly\n",
        "      device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "      # Move the model to the appropriate device\n",
        "      model.to(device)\n",
        "\n",
        "      # Process the input text\n",
        "      inputs = processor(text=input_text, src_lang=input_language, return_tensors=\"pt\").to(device)\n",
        "\n",
        "      # Perform translation\n",
        "      with torch.no_grad():\n",
        "          outputs = model.generate(**inputs, tgt_lang=output_language, generate_speech=False)\n",
        "\n",
        "      # Extract the first sequence and decode it\n",
        "      translated_text = processor.decode(outputs[0].tolist()[0], skip_special_tokens=True)\n",
        "\n",
        "      # Print the translated text\n",
        "      print(f\"\\nTranslated text: {translated_text}\")\n",
        "    elif model_index == 1:\n",
        "      src_text = [\">>\" + output_language + \"<<\" + input_text]\n",
        "      model_name = \"Helsinki-NLP/opus-mt-en-roa\"\n",
        "      tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
        "      print(tokenizer.supported_language_codes)\n",
        "\n",
        "      model = MarianMTModel.from_pretrained(model_name)\n",
        "      translated = model.generate(**tokenizer(src_text, return_tensors=\"pt\", padding=True))\n",
        "      translations = [tokenizer.decode(t, skip_special_tokens=True) for t in translated]\n",
        "      for i, translation in enumerate(translations):\n",
        "          print(f\"Translated: {translation}\\n\")\n",
        "    elif model_index == 2:\n",
        "      # Load the tokenizer and the model\n",
        "      tokenizer = AutoTokenizer.from_pretrained(\"facebook/nllb-200-distilled-600M\")\n",
        "      model = AutoModelForSeq2SeqLM.from_pretrained(\"facebook/nllb-200-distilled-600M\")\n",
        "\n",
        "      source_text = \">>\" + output_language + \"<<\" + input_text\n",
        "\n",
        "      # Tokenize the input text\n",
        "      input_ids = tokenizer(source_text, return_tensors=\"pt\").input_ids  # Batch size 1\n",
        "\n",
        "      # Generate the translation (you can specify the target language)\n",
        "      translated_tokens = model.generate(input_ids=input_ids, max_length=50, num_beams=4, early_stopping=True)\n",
        "\n",
        "      # Decode the translated tokens to get the translated text\n",
        "      translated_text = tokenizer.decode(translated_tokens[0], skip_special_tokens=True)\n",
        "      print(translated_text)\n",
        "    elif model_index == 3:\n",
        "      model_name = 'google/madlad400-10b-mt'\n",
        "      model = T5ForConditionalGeneration.from_pretrained(model_name, device_map=\"auto\")\n",
        "      tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
        "\n",
        "      text = \"<2\" + output_language + \">\" +  input_text\n",
        "      input_ids = tokenizer(text, return_tensors=\"pt\").input_ids.to(model.device)\n",
        "      outputs = model.generate(input_ids=input_ids)\n",
        "\n",
        "      translated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "      print(translated_text)\n",
        "\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    translate_text()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GPVbg4HCSKHr",
        "outputId": "9022bf31-5a5c-4d4a-d88a-2e23093a51c6"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter the text you want to translate: It was a good project\n",
            "Detected input language: en\n",
            "Enter the language code you want to translate to (e.g., 'fra' for French): pes\n",
            "\n",
            "Available models:\n",
            "1. facebook/hf-seamless-m4t-medium\n",
            "2. Helsinki-NLP/opus-mt-en-roa\n",
            "3. facebook/nllb-200-distilled-600M\n",
            "4. google/madlad400-10b-mt\n",
            "Select the translation model by number: 1\n",
            "\n",
            "You have selected the model: facebook/hf-seamless-m4t-medium\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Translated text: این یک پروژه خوب بود\n"
          ]
        }
      ]
    }
  ]
}